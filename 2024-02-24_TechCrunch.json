{
  "source": "TechCrunch",
  "date": "2024-02-24",
  "no_of_articles": 4,
  "articles": [
    {
      "title": "This Week in AI: Addressing racism in AI image generators",
      "content": "\nKeeping up with an industry as fast-moving as AI is a tall order. So until an AI can do it for you, here’s a handy roundup of recent stories in the world of machine learning, along with notable research and experiments we didn’t cover on their own.\nThis week in AI, Google paused its AI chatbot Gemini’s ability to generate images of people after a segment of users complained about historical inaccuracies. Told to depict “a Roman legion,” for instance, Gemini would show an anachronistic, cartoonish group of racially diverse foot soldiers while rendering “Zulu warriors” as Black.\nIt appears that Google — like some other AI vendors, including OpenAI — had implemented clumsy hardcoding under the hood to attempt to “correct” for biases in its model. In response to prompts like “show me images of only women” or “show me images of only men,” Gemini would refuse, asserting such images could “contribute to the exclusion and marginalization of other genders.” Gemini was also loath to generate images of people identified solely by their race — e.g. “white people” or “black people” — out of ostensible concern for “reducing individuals to their physical characteristics.”\nRight wingers have latched on to the bugs as evidence of a “woke” agenda being perpetuated by the tech elite. But it doesn’t take Occam’s razor to see the less nefarious truth: Google, burned by its tools’ biases before (see: classifying Black men as gorillas, mistaking thermal guns in Black people’s hands as weapons, etc.), is so desperate to avoid history repeating itself that it’s manifesting a less biased world in its image-generating models — however erroneous.\nIn her best-selling book “White Fragility,” anti-racist educator Robin DiAngelo writes about how the erasure of race — “color blindness,” by another phrase — contributes to systemic racial power imbalances rather than mitigating or alleviating them. By purporting to “not see color” or reinforcing the notion that simply acknowledging the struggle of people of other races is sufficient to label oneself “woke,” people perpetuate harm by avoiding any substantive conservation on the topic, DiAngelo says.\nGoogle’s ginger treatment of race-based prompts in Gemini didn’t avoid the issue, per se — but disingenuously attempted to conceal the worst of the model’s biases. One could argue (and many have) that these biases shouldn’t be ignored or glossed over, but addressed in the broader context of the training data from which they arise — i.e. society on the world wide web.\nYes, the data sets used to train image generators generally contain more white people than Black people, and yes, the images of Black people in those data sets reinforce negative stereotypes. That’s why image generators sexualize certain women of color, depict white men in positions of authority and generally favor wealthy Western perspectives.\nSome may argue that there’s no winning for AI vendors. Whether they tackle — or choose not to tackle — models’ biases, they’ll be criticized. And that’s true. But I posit that, either way, these models are lacking in explanation — packaged in a fashion that minimizes the ways in which their biases manifest.\nWere AI vendors to address their models’ shortcomings head on, in humble and transparent language, it’d go a lot further than haphazard attempts at “fixing” what’s essentially unfixable bias. We all have bias, the truth is — and we don’t treat people the same as a result. Nor do the models we’re building. And we’d do well to acknowledge that.\n\n‘Embarrassing and wrong’: Google admits it lost control of image-generating AI\n\nHere are some other AI stories of note from the past few days:\n\nWomen in AI: TechCrunch launched a series highlighting notable women in the field of AI. Read the list here.\nStable Diffusion v3: Stability AI has announced Stable Diffusion 3, the latest and most powerful version of the company’s image-generating AI model, based on a new architecture.\nChrome gets GenAI: Google’s new Gemini-powered tool in Chrome allows users to rewrite existing text on the web — or generate something completely new.\nBlacker than ChatGPT: Creative ad agency McKinney developed a quiz game, Are You Blacker than ChatGPT?, to shine a light on AI bias.\nCalls for laws: Hundreds of AI luminaries signed a public letter earlier this week calling for anti-deepfake legislation in the U.S.\nMatch made in AI: OpenAI has a new customer in Match Group, the owner of apps including Hinge, Tinder and Match, whose employees will use OpenAI’s AI tech to accomplish work-related tasks.\nDeepMind safety: DeepMind, Google’s AI research division, has formed a new org, AI Safety and Alignment, made up of existing teams working on AI safety but also broadened to encompass new, specialized cohorts of GenAI researchers and engineers.\nOpen models: Barely a week after launching the latest iteration of its Gemini models, Google released Gemma, a new family of lightweight open-weight models.\nHouse task force: The U.S. House of Representatives has founded a task force on AI that — as Devin writes — feels like a punt after years of indecision that show no sign of ending.\n\nMore machine learnings\nAI models seem to know a lot, but what do they actually know? Well, the answer is nothing. But if you phrase the question slightly differently… they do seem to have internalized some “meanings” that are similar to what humans know. Although no AI truly understands what a cat or a dog is, could it have some sense of similarity encoded in its embeddings of those two words that is different from, say, cat and bottle? Amazon researchers believe so.\nTheir research compared the “trajectories” of similar but distinct sentences, like “the dog barked at the burglar” and “the burglar caused the dog to bark,” with those of grammatically similar but different sentences, like “a cat sleeps all day” and “a girl jogs all afternoon.” They found that the ones humans would find similar were indeed internally treated as more similar despite being grammatically different, and vice versa for the grammatically similar ones. OK, I feel like this paragraph was a little confusing, but suffice it to say that the meanings encoded in LLMs appear to be more robust and sophisticated than expected, not totally naive.\nNeural encoding is proving useful in prosthetic vision, Swiss researchers at EPFL have found. Artificial retinas and other ways of replacing parts of the human visual system generally have very limited resolution due to the limitations of microelectrode arrays. So no matter how detailed the image is coming in, it has to be transmitted at a very low fidelity. But there are different ways of downsampling, and this team found that machine learning does a great job at it.\nImage Credits: EPFL\n“We found that if we applied a learning-based approach, we got improved results in terms of optimized sensory encoding. But more surprising was that when we used an unconstrained neural network, it learned to mimic aspects of retinal processing on its own,” said Diego Ghezzi in a news release. It does perceptual compression, basically. They tested it on mouse retinas, so it isn’t just theoretical.\nAn interesting application of computer vision by Stanford researchers hints at a mystery in how children develop their drawing skills. The team solicited and analyzed 37,000 drawings by kids of various objects and animals, and also (based on kids’ responses) how recognizable each drawing was. Interestingly, it wasn’t just the inclusion of signature features like a rabbit’s ears that made drawings more recognizable by other kids.\n\n“The kinds of features that lead drawings from older children to be recognizable don’t seem to be driven by just a single feature that all the older kids learn to include in their drawings. It’s something much more complex that these machine learning systems are picking up on,” said lead researcher Judith Fan.\nChemists (also at EPFL) found that LLMs are also surprisingly adept at helping out with their work after minimal training. It’s not just doing chemistry directly, but rather being fine-tuned on a body of work that chemists individually can’t possibly know all of. For instance, in thousands of papers there may be a few hundred statements about whether a high-entropy alloy is single or multiple phase (you don’t have to know what this means — they do). The system (based on GPT-3) can be trained on this type of yes/no question and answer, and soon is able to extrapolate from that.\nIt’s not some huge advance, just more evidence that LLMs are a useful tool in this sense. “The point is that this is as easy as doing a literature search, which works for many chemical problems,” said researcher Berend Smit. “Querying a foundational model might become a routine way to bootstrap a project.”\nLast, a word of caution from Berkeley researchers, though now that I’m reading the post again I see EPFL was involved with this one too. Go Lausanne! The group found that imagery found via Google was much more likely to enforce gender stereotypes for certain jobs and words than text mentioning the same thing. And there were also just way more men present in both cases.\nNot only that, but in an experiment, they found that people who viewed images rather than reading text when researching a role associated those roles with one gender more reliably, even days later. “This isn’t only about the frequency of gender bias online,” said researcher Douglas Guilbeault. “Part of the story here is that there’s something very sticky, very potent about images’ representation of people that text just doesn’t have.”\nWith stuff like the Google image generator diversity fracas going on, it’s easy to lose sight of the established and frequently verified fact that the source of data for many AI models shows serious bias, and this bias has a real effect on people.\n",
      "image_links": [
        "https://techcrunch.com/wp-content/uploads/2019/05/GettyImages-1062086882.jpg?w=600",
        "https://techcrunch.com/wp-content/uploads/2024/02/neural-downsample-epfl.jpg",
        "https://techcrunch.com/wp-content/uploads/2024/02/Drawings_Examples.webp"
      ],
      "link": "https://techcrunch.com/2024/02/24/this-week-in-ai-addressing-racism-in-ai-image-generators/",
      "category": "artificial-intelligence",
      "Summary": "- Google paused its AI chatbot Gemini's ability to generate images of people after users complained about historical inaccuracies and biases in the images.\n\n- Google attempted to \"correct\" for biases in its model by implementing hardcoding, which resulted in the model refusing to generate images of people identified solely by their race.\n\n- Some argue that AI vendors should address their models' shortcomings head-on and provide explanations for their biases, rather than attempting to conceal them.\n\n- Amazon researchers found that AI models seem to have internalized some \"meanings\" that are similar to what humans know, and that the meanings encoded in LLMs appear to be more robust and sophisticated than expected.\n\n- Stanford researchers analyzed children's drawings and found that the kinds of features that lead drawings from older children to be recognizable don't seem to be driven by just a single feature that all the older kids learn to include in their drawings.",
      "revised_title": "Addressing Racial Bias in AI Image Generators: A Call for Transparency and Accountability"
    },
    {
      "title": "Miranda Bogen is creating solutions to help govern AI",
      "content": "\nTo give AI-focused women academics and others their well-deserved — and overdue — time in the spotlight, TechCrunch is launching a series of interviews focusing on remarkable women who’ve contributed to the AI revolution. We’ll publish several pieces throughout the year as the AI boom continues, highlighting key work that often goes unrecognized. Read more profiles here.\nMiranda Bogen is the founding director of the Center of Democracy and Technology’s AI Governance Lab, where she works to help create solutions that can effectively regulate and govern AI systems. She helped guide responsible AI strategies at Meta and previously worked as a senior policy analyst at the organization Uptown, which seeks to use tech to advance equity and justice.\nBriefly, how did you get your start in AI? What attracted you to the field?\nI was drawn to work on machine learning and AI by seeing the way these technologies were colliding with fundamental conversations about society — values, rights, and which communities get left behind. My early work exploring the intersection of AI and civil rights reinforced for me that AI systems are far more than technical artifacts; they are systems that both shape and are shaped by their interaction with people, bureaucracies, and policies. I’ve always been adept at translating between technical and non-technical contexts, and I was energized by the opportunity to help break through the appearance of technical complexity to help communities with different kinds of expertise shape the way AI is built from the ground up.\nWhat work are you most proud of (in the AI field)?\nWhen I first started working in this space, many folks still needed to be convinced AI systems could result in discriminatory impact for marginalized populations, let alone that anything needed to be done about those harms. While there is still too wide a gap between the status quo and a future where biases and other harms are tackled systematically, I’m gratified that the research my collaborators and I conducted on discrimination in personalized online advertising and my work within the industry on algorithmic fairness helped lead to meaningful changes to Meta’s ad delivery system and progress toward reducing disparities in access to important economic opportunities.\nHow do you navigate the challenges of the male-dominated tech industry and, by extension, the male-dominated AI industry?\nI’ve been lucky to work with phenomenal colleagues and teams who have been generous with both opportunities and sincere support, and we tried to bring that energy into any room we found ourselves in. In my most recent career transition, I was delighted that nearly all of my options involved working on teams or within organizations led by phenomenal women, and I hope the field continues to lift up the voices of those who haven’t traditionally been centered in technology-oriented conversations.\nWhat advice would you give to women seeking to enter the AI field?\nThe same advice I give to anyone who asks: find supportive managers, advisors, and teams who energize and inspire you, who value your opinion and perspective, and who put themselves on the line to stand up for you and your work.\nWhat are some of the most pressing issues facing AI as it evolves?\nThe impacts and harms AI systems are already having on people are well-known at this point, and one of the biggest pressing challenges is moving beyond describing the problem to developing robust approaches for systematically addressing those harms and incentivizing their adoption. We launched the AI Governance Lab at CDT to drive progress in both directions.\nWhat are some issues AI users should be aware of?\nFor the most part, AI systems are still missing seat belts, airbags, and traffic signs, so proceed with caution before using them for consequential tasks.\nWhat is the best way to responsibly build AI?\nThe best way to responsibly build AI is with humility. Consider how the success of the AI system you are working on has been defined, who that definition serves, and what context may be missing. Think about for whom the system might fail and what will happen if it does. And build systems not just with the people who will use them but with the communities who will be subject to them.\nHow can investors better push for responsible AI?\nInvestors need to create room for technology builders to move more deliberately before rushing half-baked technologies to market. Intense competitive pressure to release the newest, biggest, and shiniest new AI models is leading to concerning underinvestment in responsible practices. While uninhibited innovation sings a tempting siren song, it is a mirage that will leave everyone worse off.\nAI is not magic; it’s just a mirror that is being held up to society. If we want it to reflect something different, we’ve got work to do.\n",
      "image_links": [
        "https://techcrunch.com/wp-content/uploads/2024/02/women-in-ai-.jpg?w=711"
      ],
      "link": "https://techcrunch.com/2024/02/24/miranda-bogen-is-creating-solutions-to-help-govern-ai/",
      "category": "artificial-intelligence",
      "Summary": "- Miranda Bogen is the founding director of the Center of Democracy and Technology's AI Governance Lab, where she works to create solutions for regulating and governing AI systems.\n\n- Bogen was drawn to AI because of its impact on society, values, rights, and marginalized communities.\n\n- She is proud of her work on discrimination in personalized online advertising and algorithmic fairness, which led to changes in Meta's ad delivery system and progress in reducing disparities in access to economic opportunities.\n\n- Bogen advises women seeking to enter the AI field to find supportive managers, advisors, and teams who value their opinion and perspective.\n\n- She believes the best way to responsibly build AI is with humility, considering how the system's success is defined, who it serves, and who it might fail.",
      "revised_title": "Interview with Miranda Bogen: Navigating the Challenges of AI Governance and Building Responsible AI Systems"
    },
    {
      "title": "Byju’s founder, ousted by shareholders, says rumors of his firing ‘greatly exaggerated’",
      "content": "\nByju Raveendran, the founder of eponymous edtech group Byju’s, told employees on Saturday that he continues to remain the chief executive of the startup and that rumors of his firing have been “greatly exaggerated,” a day after a shareholder group voted to remove him at an emergency general meeting.\nIn a 758-word letter, content of which was reviewed by TechCrunch, Raveendran claimed that the shareholders violated several “essential” local rules.\nThe shareholder group, which included Prosus Ventures and Peak XV Ventures, said in a statement on Friday that the investors “unanimously passed” the resolutions that seek to enact, among other things, addressing governance, compliance issues, financial mismanagement, reconstitution of its board and a change in leadership “so that it is no longer controlled by the founders of T&L.”\nAt stake is the future of the Bengaluru-headquartered startup, which was once the country’s most valuable.\nRaveendran claimed in the letter that the extraordinary general meeting lacked the minimum quorum and failed to win majority support for proposed resolutions. Raveendran claimed the EGM was convened without adhering to the procedures set out by law and only 35 of Byju’s 170 total shareholders attended, representing around 45% ownership in the company.\n“This means that whatever was decided in that meeting does not count, because it didn’t stick to the established rules. Regardless of the relentless trial by the media, I firmly believe that the truth will inevitably prevail,” he wrote in the letter to employees.\nThe cash-starved startup, which has been hunting for new funding for over a year, late last month launched a rights issue, where it seeks to raise about $200 million. The rights issue resets the startup’s valuation, once at $22 billion, to about $25 million.\n“Our rights issue has seen an overwhelming response. In fact, such has been the scale of its success that even those who were sitting on the fence are now rushing to get a piece of the action. This momentum is irreversible, and our comeback is now inevitable,” Raveendran told employees.\n“It should be clear from the above and the various news reports, which paint a contradictory picture of the effect of yesterday’s meeting, that these minority shareholders are intent on spreading misinformation in the media. The Company will not stoop to their level and engage in a media war. We are confident that their actions will ultimately fail, and the Company’s position will prevail.”\n",
      "image_links": [
        "https://techcrunch.com/wp-content/uploads/2021/03/GettyImages-1132893022.jpg?w=600"
      ],
      "link": "https://techcrunch.com/2024/02/24/byjus-founder-ousted-by-shareholders-says-rumors-of-his-firing-greatly-exaggerated/",
      "category": "startups",
      "Summary": "- Byju Raveendran, founder of Byju's, denies rumors of his firing after a shareholder group voted to remove him at an emergency general meeting.\n\n- Raveendran claims the meeting lacked the minimum quorum and failed to win majority support for proposed resolutions, and that it was convened without adhering to legal procedures.\n\n- The shareholder group, including Prosus Ventures and Peak XV Ventures, sought to address governance, compliance issues, financial mismanagement, board reconstitution, and a change in leadership.\n\n- Byju's, once India's most valuable startup, is cash-starved and has been seeking new funding for over a year.\n\n- Raveendran launched a rights issue to raise about $200 million, resetting the startup's valuation from $22 billion to $25 million.",
      "revised_title": "Byju's Founder Denies Rumors of His Ouster, Claims Shareholder Meeting Lacked Quorum"
    },
    {
      "title": "Stellantis CEO says there’s still life in Waymo deal for self-driving delivery vans",
      "content": "\nStellantis, the automaker that owns 14 brands including Chrysler, Jeep and Ram, and autonomous vehicle technology company Waymo are not only still working together, the companies are deepening the partnership, CEO Carlos Tavares told TechCrunch in a recent interview.\nThis “deepened” partnership will focus on commercial self-driving Ram delivery vans, a target that was first announced in 2020 and promptly faded from public view. Discussions on this “improved” deal have focused, in part, on a crux around driverless delivery: how does the package get from the vehicle to the customer?\n“When you reach the destination, how do you take the parcel out of the van?” Tavares said in a wide-ranging interview. “This has been a point of discussion that doesn’t seem easy to solve and we are now upgrading our collaboration deal with them to take that into consideration.”\n“At the same time, we understand their needs and there are a lot of things that we can do for them in terms of engineering,” he said, adding it is too soon to share details. “But I would say that the partnership with Waymo is getting deeper. And I think, more exciting.”\nTavares played coy on the important what, where and when details. But he did add that he expected to be able to share more “possibly by summer.”\nA Waymo spokesperson confirmed that the company continues to look at ways to deepen its relationship with Stellantis, but didn’t share any other details or if progress had been made.\nTavares’ comments suggest the company has more than a passing interest in reviving a deal\nthat appeared destined to fizzle out as so many other autonomous vehicle-OEM partnerships have in the past two years.\nEven if the two companies do cement a broader deal, there is still the very real challenge of executing it.\nWaymo, which is owned by Google parent-company Alphabet, currently doesn’t operate a commercial delivery service using its self-driving vehicles. Last summer, it shuttered its self-driving trucks program, Waymo Via, to put all of its resources into scaling the robotaxi service.\nIn May 2023, Waymo and Uber agreed to a multi-year strategic partnership to allow Uber users to hail a driverless vehicle via the app in Phoenix. That deal did include a future plan to include delivery via Uber Eats, but as of today, it has not launched, according to a Waymo spokesperson.\nThe two companies have been partners since 2016 when a deal was struck to supply Waymo with thousands of custom Chrysler Pacifica Hybrid minivans that would become the first driverless vehicles to launch.\nUnder the deal, Fiat Chrysler — now known as Stellantis — would handle the manufacturing and provide Waymo with minivans that built in redundancies designed for autonomous driving.\nWaymo never got close to the 62,000-minivan order it agreed to in 2018 as part of an expanded partnership with Fiat Chrysler. Hundreds, not thousands, of minivans were delivered to Waymo. But the minivan did become a critical part of its commercialization plan and over its lifespan the fleet provided tens of thousands of rides to the public, according to the company. (Waymo has never revealed detailed figures of its minivan fleet beyond that its total global fleet is somewhere around 700 vehicles.)\nWaymo ended the Chrysler Pacifica program in May 2023. Today, its robotaxi service uses all-electric Jaguar I-Pace vehicles.\n",
      "image_links": [
        "https://techcrunch.com/wp-content/uploads/2019/10/Waymo-LOGO-door.jpg?w=666"
      ],
      "link": "https://techcrunch.com/2024/02/24/stellantis-ceo-says-theres-still-life-in-waymo-deal-for-self-driving-delivery-vans/",
      "category": "transportation",
      "Summary": "- Stellantis and Waymo are deepening their partnership to focus on commercial self-driving Ram delivery vans.\n\n- The partnership aims to address the challenge of delivering packages from the vehicle to the customer.\n\n- Stellantis will provide engineering support to Waymo, and the companies expect to share more details by summer.\n\n- Waymo currently doesn't operate a commercial delivery service using its self-driving vehicles and has shuttered its self-driving trucks program.\n\n- Waymo and Uber have a strategic partnership to allow Uber users to hail a driverless vehicle in Phoenix, with plans to include delivery via Uber Eats in the future.",
      "revised_title": "Stellantis and Waymo Deepen Partnership for Self-Driving Delivery Vans"
    }
  ]
}